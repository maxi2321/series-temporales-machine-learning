{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.append(os.path.join('..','..','libs'))\n",
    "from read_smn import read_smn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN en predicciones\n",
    "\n",
    "Vamos a ver cómo utilizar RNNs para predecir datos futuros. En este caso utilizaremos los datos del SMN para poder comparar con lo estudiado de MLP\n",
    "\n",
    "## Problema univariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos los datos\n",
    "readr = read_smn(os.path.join('..','..','Data','junio-SMN','horario'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos sólo SALTA\n",
    "tstamps, data = readr.filter_by_station('SALTA')\n",
    "data = data[:,0]  # solo la temperatura\n",
    "train = data[:600] # split train y test\n",
    "test = data[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "class DataPipeline:\n",
    "    def __init__(self, train_data, lookback) -> None:\n",
    "        self.train_data = train_data.reshape(-1,1)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.train_data = self.scaler.fit_transform(self.train_data)\n",
    "        self.lookback = lookback\n",
    "        self.train_x, self.train_y = self.split_series(self.train_data)\n",
    "\n",
    "    def transform(self, data):\n",
    "        '''wrapper to transform new data'''\n",
    "        return self.scaler.transform(data)\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        '''wraper to inverse transform'''\n",
    "        return self.scaler.inverse_transform(data)\n",
    "    \n",
    "    def split_series(self, data):\n",
    "        ''' tenemos una unica serie, entonces la partimos en lookback pedacitos\n",
    "            para crear muchas series cortas de longitud lookback\n",
    "        '''\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(self.train_data.shape[0]-self.lookback):\n",
    "            x.append(self.train_data[i:i+self.lookback]) # creamos directamente los arrays con la forma que queremos\n",
    "            y.append(self.train_data[i+self.lookback])\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def perform_transformations(self, data):\n",
    "        '''\n",
    "        This method perform transformations for any single valued series\n",
    "        series length must be larger than lookback\n",
    "        '''\n",
    "        assert len(data) > self.lookback\n",
    "        data = self.transform(data)\n",
    "        x, y = self.split_series(data)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos un dataset y dataloader para hacer todo mas simple con pytorch\n",
    "# como las series ya traen la forma que necesitamos, \n",
    "# solo hay que hacer un cast de numpy a pytorch\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.X = x\n",
    "        self.Y = y\n",
    "        self.X = torch.from_numpy(self.X).float()\n",
    "        self.Y = torch.from_numpy(self.Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 588 series temporales de largo 12\n"
     ]
    }
   ],
   "source": [
    "# vamos a crear un objeto para preprocesar los datos\n",
    "lookback = 12\n",
    "\n",
    "pipe = DataPipeline(train_data= train, lookback= lookback)\n",
    "\n",
    "# creamos los datasets y dataloaders en un diccionario para manejar mas simple\n",
    "\n",
    "dataset = DataSet(pipe.train_x, pipe.train_y)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 64, shuffle= True)\n",
    "\n",
    "print(f'Tenemos {len(dataset)} series temporales de largo {lookback}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(1, 1, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Definimos la rnn mas pequeña posible\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn  = torch.nn.RNN(\n",
    "                                input_size  = 1,    # la serie es univariada y entra una sola feature\n",
    "                                hidden_size = 1,\n",
    "                                num_layers  = 1,\n",
    "                                batch_first = True\n",
    "                                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, h = self.rnn(x)\n",
    "        return x, h     # retornamos todos los valores\n",
    "\n",
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parametros de la rnn\n",
      "--------------------------------------------------\n",
      "weights: input-hidden -layer:0 = Parameter containing:\n",
      "tensor([[-0.5974]], requires_grad=True)\n",
      "weights: hidden-hidden -layer:0 = Parameter containing:\n",
      "tensor([[-0.7138]], requires_grad=True)\n",
      "bias: input-hidden -layer:0 = Parameter containing:\n",
      "tensor([0.2100], requires_grad=True)\n",
      "bias: hidden-hidden -layer:0 = Parameter containing:\n",
      "tensor([-0.7336], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('parametros de la rnn')\n",
    "print('-'*50)\n",
    "print(f'weights: input-hidden -layer:0 = {rnn.rnn.weight_ih_l0}')\n",
    "print(f'weights: hidden-hidden -layer:0 = {rnn.rnn.weight_hh_l0}')\n",
    "print(f'bias: input-hidden -layer:0 = {rnn.rnn.bias_ih_l0}')\n",
    "print(f'bias: hidden-hidden -layer:0 = {rnn.rnn.bias_hh_l0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward del modelo:\n",
      "tensor([[[-0.8079],\n",
      "         [-0.4962],\n",
      "         [-0.6450],\n",
      "         [-0.5787],\n",
      "         [-0.6093],\n",
      "         [-0.5954],\n",
      "         [-0.6018],\n",
      "         [-0.5989],\n",
      "         [-0.6002],\n",
      "         [-0.5996],\n",
      "         [-0.5999],\n",
      "         [-0.5998]]], grad_fn=<TransposeBackward1>)\n",
      "Estado de las capas ocultas al final:\n",
      "tensor([[[-0.5998]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# veamos como se comporta nuestra red super simple\n",
    "x = np.ones(shape=(1,lookback,1))   # hacemos un vector de unos\n",
    "x = torch.from_numpy(x).float()     # lo pasamos a \n",
    "y, h = rnn(x)\n",
    "print('forward del modelo:') # una salida por cada paso temporal\n",
    "print(y)\n",
    "print('Estado de las capas ocultas al final:') # el estado de las capas ocultas al final de la serie\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, epochs = 10, eval = False):\n",
    "    '''\n",
    "    Funcion para entrenar el modelo model utilizando un dataloader\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    history = []\n",
    "    for epoch in range(1,epochs+1):\n",
    "        model.train()   # ponemos el modelo para ser entrenado\n",
    "        train_h = [] \n",
    "        \n",
    "        # leer los datos en el dataloader es muy simple (recorrer por batches)! \n",
    "        for x_b, y_b in dataloader['train']:\n",
    "\n",
    "            # ponemos los gradientes a cero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "\n",
    "            # calculamos los gradientes\n",
    "            loss.backward()\n",
    "            \n",
    "            # actualizamos todos los pesos\n",
    "            optimizer.step()\n",
    "            train_h.append(loss.item())\n",
    "\n",
    "        if eval:\n",
    "            model.eval() # no estamos entrenando\n",
    "            test_h = [] \n",
    "            with torch.no_grad():  # no vamos a hacer backward, solo ver la metrica sobre el test\n",
    "                for x_b, y_b in dataloader['valid']:\n",
    "                    y_pred = model(x_b)\n",
    "                    loss = criterion(y_pred, y_b)\n",
    "                    test_h.append(loss.item())\n",
    "        if (epoch%2 == 0):\n",
    "            if eval:\n",
    "                print(f'epoch: {epoch}/{epochs} - train loss: {np.mean(train_h):.3f} - valid loss: {np.mean(test_h):.3f}')\n",
    "                history.append([np.mean(train_h), np.mean(test_h)])\n",
    "            else:\n",
    "                print(f'epoch: {epoch}/{epochs} - train loss: {np.mean(train_h):.3f}')\n",
    "                history.append([np.mean(train_h)])\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un modelo que devuelva solo el ultimo valor (MANY TO ONE)\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn  = torch.nn.RNN(\n",
    "                                input_size  = 1,    # la serie es univariada y entra una sola feature\n",
    "                                hidden_size = 1,\n",
    "                                num_layers  = 1,\n",
    "                                batch_first = True\n",
    "                                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        return x[:,-1]     # retornamos el último valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2/10 - train loss: 0.082\n",
      "epoch: 4/10 - train loss: 0.067\n",
      "epoch: 6/10 - train loss: 0.053\n",
      "epoch: 8/10 - train loss: 0.043\n",
      "epoch: 10/10 - train loss: 0.037\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "his = fit(rnn,{'train': dataloader})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
