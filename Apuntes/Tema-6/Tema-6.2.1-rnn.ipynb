{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.append(os.path.join('..','..','libs'))\n",
    "from read_smn import read_smn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN en predicciones\n",
    "\n",
    "Vamos a ver cómo utilizar RNNs para predecir datos futuros. En este caso utilizaremos los datos del SMN para poder comparar con lo estudiado de MLP\n",
    "\n",
    "## Problema univariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos los datos\n",
    "readr = read_smn(os.path.join('..','..','Data','junio-SMN','horario'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos sólo SALTA\n",
    "tstamps, data = readr.filter_by_station('SALTA')\n",
    "data = data[:,0]  # solo la temperatura\n",
    "train = data[:600] # split train y test\n",
    "test = data[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "class DataPipeline:\n",
    "    def __init__(self, train_data, lookback) -> None:\n",
    "        self.train_data = train_data.reshape(-1,1)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.train_data = self.scaler.fit_transform(self.train_data)\n",
    "        self.lookback = lookback\n",
    "        self.train_x, self.train_y = self.split_series(self.train_data)\n",
    "\n",
    "    def transform(self, data):\n",
    "        '''wrapper to transform new data'''\n",
    "        return self.scaler.transform(data)\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        '''wraper to inverse transform'''\n",
    "        return self.scaler.inverse_transform(data)\n",
    "    \n",
    "    def split_series(self, data):\n",
    "        ''' tenemos una unica serie, entonces la partimos en lookback pedacitos\n",
    "            para crear muchas series cortas de longitud lookback\n",
    "        '''\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(self.train_data.shape[0]-self.lookback):\n",
    "            x.append(self.train_data[i:i+self.lookback]) # creamos directamente los arrays con la forma que queremos\n",
    "            y.append(self.train_data[i+self.lookback])\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def perform_transformations(self, data):\n",
    "        '''\n",
    "        This method perform transformations for any single valued series\n",
    "        series length must be larger than lookback\n",
    "        '''\n",
    "        assert len(data) > self.lookback\n",
    "        data = self.transform(data)\n",
    "        x, y = self.split_series(data)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos un dataset y dataloader para hacer todo mas simple con pytorch\n",
    "# como las series ya traen la forma que necesitamos, \n",
    "# solo hay que hacer un cast de numpy a pytorch\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.X = x\n",
    "        self.Y = y\n",
    "        self.X = torch.from_numpy(self.X).float()\n",
    "        self.Y = torch.from_numpy(self.Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 588 series temporales de largo 12\n"
     ]
    }
   ],
   "source": [
    "# vamos a crear un objeto para preprocesar los datos\n",
    "lookback = 12\n",
    "\n",
    "pipe = DataPipeline(train_data= train, lookback= lookback)\n",
    "\n",
    "# creamos los datasets y dataloaders en un diccionario para manejar mas simple\n",
    "\n",
    "dataset = DataSet(pipe.train_x, pipe.train_y)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 64, shuffle= True)\n",
    "\n",
    "print(f'Tenemos {len(dataset)} series temporales de largo {lookback}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(1, 1, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Definimos la rnn mas pequeña posible\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn  = torch.nn.RNN(\n",
    "                                input_size  = 1,    # la serie es univariada y entra una sola feature\n",
    "                                hidden_size = 1,\n",
    "                                num_layers  = 1,\n",
    "                                batch_first = True\n",
    "                                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, h = self.rnn(x)\n",
    "        return x, h     # retornamos todos los valores\n",
    "\n",
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parametros de la rnn\n",
      "--------------------------------------------------\n",
      "weights: input-hidden -layer:0 = Parameter containing:\n",
      "tensor([[-0.5974]], requires_grad=True)\n",
      "weights: hidden-hidden -layer:0 = Parameter containing:\n",
      "tensor([[-0.7138]], requires_grad=True)\n",
      "bias: input-hidden -layer:0 = Parameter containing:\n",
      "tensor([0.2100], requires_grad=True)\n",
      "bias: hidden-hidden -layer:0 = Parameter containing:\n",
      "tensor([-0.7336], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('parametros de la rnn')\n",
    "print('-'*50)\n",
    "print(f'weights: input-hidden -layer:0 = {rnn.rnn.weight_ih_l0}')\n",
    "print(f'weights: hidden-hidden -layer:0 = {rnn.rnn.weight_hh_l0}')\n",
    "print(f'bias: input-hidden -layer:0 = {rnn.rnn.bias_ih_l0}')\n",
    "print(f'bias: hidden-hidden -layer:0 = {rnn.rnn.bias_hh_l0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,lookback,\u001b[38;5;241m1\u001b[39m))   \u001b[38;5;66;03m# hacemos un vector de unos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\u001b[38;5;241m.\u001b[39mfloat()     \u001b[38;5;66;03m# lo pasamos a \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y, h \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward del modelo:\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# una salida por cada paso temporal\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n",
      "File \u001b[0;32m~/Documents/Codes/series-temporales-machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Codes/series-temporales-machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 14\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Codes/series-temporales-machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Codes/series-temporales-machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Codes/series-temporales-machine-learning/lib/python3.10/site-packages/torch/nn/modules/rnn.py:586\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 586\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    591\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    592\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "# veamos como se comporta nuestra red super simple\n",
    "x = np.ones(shape=(1,lookback,1))   # hacemos un vector de unos\n",
    "x = torch.from_numpy(x).float()     # lo pasamos a \n",
    "y, h = rnn(x)\n",
    "print('forward del modelo:') # una salida por cada paso temporal\n",
    "print(y)\n",
    "print('Estado de las capas ocultas al final:') # el estado de las capas ocultas al final de la serie\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, epochs = 10, eval = False):\n",
    "    '''\n",
    "    Funcion para entrenar el modelo model utilizando un dataloader\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    history = []\n",
    "    for epoch in range(1,epochs+1):\n",
    "        model.train()   # ponemos el modelo para ser entrenado\n",
    "        train_h = [] \n",
    "        \n",
    "        # leer los datos en el dataloader es muy simple (recorrer por batches)! \n",
    "        for x_b, y_b in dataloader['train']:\n",
    "\n",
    "            # ponemos los gradientes a cero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "\n",
    "            # calculamos los gradientes\n",
    "            loss.backward()\n",
    "            \n",
    "            # actualizamos todos los pesos\n",
    "            optimizer.step()\n",
    "            train_h.append(loss.item())\n",
    "\n",
    "        if eval:\n",
    "            model.eval() # no estamos entrenando\n",
    "            test_h = [] \n",
    "            with torch.no_grad():  # no vamos a hacer backward, solo ver la metrica sobre el test\n",
    "                for x_b, y_b in dataloader['valid']:\n",
    "                    y_pred = model(x_b)\n",
    "                    loss = criterion(y_pred, y_b)\n",
    "                    test_h.append(loss.item())\n",
    "        if (epoch%10 == 0):\n",
    "            if eval:\n",
    "                print(f'epoch: {epoch}/{epochs} - train loss: {np.mean(train_h):.3f} - valid loss: {np.mean(test_h):.3f}')\n",
    "                history.append([np.mean(train_h), np.mean(test_h)])\n",
    "            else:\n",
    "                print(f'epoch: {epoch}/{epochs} - train loss: {np.mean(train_h):.3f}')\n",
    "                history.append([np.mean(train_h)])\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un modelo que devuelva solo el ultimo valor (MANY TO ONE)\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn  = torch.nn.RNN(\n",
    "                                input_size  = 1,    # la serie es univariada y entra una sola feature\n",
    "                                hidden_size = 10,\n",
    "                                num_layers  = 10,\n",
    "                                batch_first = True\n",
    "                                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        return x[:,-1]     # retornamos el último valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10/100 - train loss: 0.043\n",
      "epoch: 20/100 - train loss: 0.037\n",
      "epoch: 30/100 - train loss: 0.039\n",
      "epoch: 40/100 - train loss: 0.037\n",
      "epoch: 50/100 - train loss: 0.021\n",
      "epoch: 60/100 - train loss: 0.007\n",
      "epoch: 70/100 - train loss: 0.004\n",
      "epoch: 80/100 - train loss: 0.003\n",
      "epoch: 90/100 - train loss: 0.003\n",
      "epoch: 100/100 - train loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "his = fit(rnn,{'train': dataloader},epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
